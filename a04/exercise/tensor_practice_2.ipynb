{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7f712f8-daa3-4dbd-9360-5d9d932c490e",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "\n",
    "The point of the exercise is to construct a simple neural character model that can predict the (n+1)th character, given the n preceding characters.\n",
    "\n",
    "Usually, such language models operate on the word level, but we use a character model because it is simpler and quicker to train and evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f1aeaf0-3a25-4467-b4d0-61535b30bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run this cell\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8b035e-b525-432a-855f-de109266da05",
   "metadata": {},
   "source": [
    "We need to map every type of input item (every character, in our case) to a unique ID number. Since we are not sure which characters will appear in our training text, we are going to create new IDs as we encounter new kinds of characters we haven't seen before.\n",
    "\n",
    "For instance, if the text begins \"Harry Potter\", we want to transform this into $[1, 2, 3, 3, 4, 5, 6, 7, 8, 8, 9, 3, ...]$, where \"H\" has ID 1, \"a\" is 2, \"r\" is 3, etc. (ID 0 is reserved for the special padding symbol, so we start numbering from 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63e58f36-84ad-45a7-9260-8e714a1a2cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to init mappings from characters to IDs and back again\n",
    "char_to_id = {}  # Dictionary to store character-to-ID mapping\n",
    "id_to_char = []  # List to store characters in their ID ordering\n",
    "PADDING_SYMBOL = '<PAD>'\n",
    "char_to_id[PADDING_SYMBOL] = 0  # ID 0 is reserved for <PAD>\n",
    "id_to_char.append(PADDING_SYMBOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e81d0246-7a35-446e-8cf6-33e130b545c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing parts in this function\n",
    "def string_to_ids(string):\n",
    "    \"\"\"\n",
    "    Translate this string into a list of character IDs.\n",
    "    The IDs will be integers 1,2,..., and created as needed.\n",
    "    \"\"\"\n",
    "    chars_ids = []  # This list will hold the result\n",
    "\n",
    "    for char in string:\n",
    "        # YOUR CODE HERE\n",
    "        if char not in char_to_id:\n",
    "            char_to_id[char] = len(char_to_id)\n",
    "            id_to_char.append(char)\n",
    "        chars_ids.append(char_to_id[char])\n",
    "    return chars_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f51c8b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'lang-eng-labs/exercise/HP_book_1.txt' # 'data/HP_book_1.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e94e393-2e80-4980-8fe7-fc1b766a3408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    contents = f.read()\n",
    "chars_ids = string_to_ids(contents)\n",
    "\n",
    "print(chars_ids[0] == chars_ids[442737])\n",
    "print(chars_ids[2677] == chars_ids[7692])\n",
    "print(chars_ids[146466] == chars_ids[312762])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947ebae1-9f82-4c44-8f36-2dc936f3f785",
   "metadata": {},
   "source": [
    "We now define a class 'CharDataset' that extends the predefined 'Dataset' class.\n",
    "\n",
    "The init function reads a training text, and slides over it, creating chunks $n$ characters long. These chunks will be our data points, and the corresponding $(n+1)$th character will be the label.\n",
    "\n",
    "For instance, if $n=4$, and the text begins \"Harry P\", which corresponds to the IDs $1,2,3,3,4,5,6$, then the first data point will be $[1,2,3,3]$ and its label is $4$, the second data point is $[2,3,3,4]$ with label $5$, and the third data point is $[3,3,4,5]$ with label $6$.\n",
    "\n",
    "To extend the 'Dataset' class, the CharDataset class has to implement the **len** and **getitem** methods, as seen below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f4d7c88-15d2-47ad-8242-4a94aa826ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing parts in this class definition\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    datapoints = []  # Each datapoint is a sequence of n characters\n",
    "    labels = []  # The corresponding label is the character that comes next\n",
    "\n",
    "    def __init__(self, file_path, n):\n",
    "        \"\"\"\n",
    "        'file_path' is the name of the training data file\n",
    "\n",
    "        'n' is the number of consecutive characters the model will look at\n",
    "        to predict which letter comes next\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            contents = f.read()\n",
    "        chars_ids = string_to_ids(contents)\n",
    "\n",
    "        # Go through the chars_ids and create data points and labels\n",
    "        # YOUR CODE HERE\n",
    "        for i in range(len(chars_ids) - n):\n",
    "            self.datapoints.append(chars_ids[i:i+n])\n",
    "            self.labels.append(chars_ids[i+n])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.datapoints)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self.datapoints)\n",
    "        return torch.tensor(self.datapoints[idx]), torch.tensor(self.labels[idx], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e73905b-05a7-4e62-9229-0bd6b90c490a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Verify\n",
    "dataset = CharDataset(data_path, 4)\n",
    "\n",
    "d402, l402 = dataset[402]\n",
    "d4002, l4002 = dataset[4002]\n",
    "d40002, l40002 = dataset[40002]\n",
    "print(l402.item() == d4002[0].item())\n",
    "print(l4002.item() == d402[3].item())\n",
    "print(l402.item() == d40002[2].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f71171f5-5c9c-4a6c-b5ab-f7112daba56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell. The function below will take care of the case of\n",
    "# sequences of unequal lengths.\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "def char_collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Pads sequences to the longest sequence in the batch.\n",
    "\n",
    "    'batch' is a list of tuples [(datapoint, label), ...]\n",
    "\n",
    "    Returns a tuple of:\n",
    "            - Padded datapoints as a tensor\n",
    "            - Labels as a tensor \n",
    "    \"\"\"\n",
    "    datapoints, labels = zip(*batch)\n",
    "    padded_datapoints = pad_sequence(datapoints, batch_first=True)\n",
    "    return padded_datapoints, torch.tensor(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254e6c95-d1bd-4383-8e54-ce0ec1158130",
   "metadata": {},
   "source": [
    "#### Create a neural network according to the following specification:\n",
    "\n",
    "The hyperparameters are:\n",
    "\n",
    "- n -- the number of characters to input (to predict character n+1)\n",
    "- h -- the number of neurons in the hidden layer\n",
    "- v -- the number of unique characters\n",
    "\n",
    "The network should have:\n",
    "\n",
    "1. an embedding layer, mapping character IDs to h-dimensional vectors\n",
    "2. a hidden layer with a linear transformation of size $(nh)\\times(nh)$, followed by a tanh application\n",
    "3. a final layer with a linear transformation of size $(nh)\\times v$\n",
    "\n",
    "The input to the forward pass is a tensor of character IDs $x$ of shape $(\\mathtt{batch\\_size} \\times n)$. The forward pass should:\n",
    "\n",
    "1. Map $x$ to a tensor of character embeddings of shape $(\\mathtt{batch\\_size} \\times n \\times h)$\n",
    "2. Reshape that tensor to shape $(\\mathtt{batch\\_size} \\times nh)$\n",
    "3. Apply the hidden layer (linear transformation and the tanh operation)\n",
    "4. Apply the final layer\n",
    "5. Return the result of the last operation\n",
    "\n",
    "Before starting the implementation, have a look at the documentation for:\n",
    "\n",
    "- `torch.nn.Embedding`\n",
    "- `torch.nn.Linear`\n",
    "- `torch.tanh`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "392ba45c-1a18-4e81-8d7f-7ea3d9f33470",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharModel(nn.Module):\n",
    "\n",
    "    def __init__(self, n, h, v):\n",
    "        super(CharModel, self).__init__()\n",
    "        # YOUR CODE HERE\n",
    "        self.embedding = nn.Embedding(v, h)\n",
    "        self.hidden = nn.Sequential(\n",
    "            nn.Linear(n*h, n*h),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.fc = nn.Linear(n*h, v)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # YOUR CODE HERE\n",
    "        x = self.embedding(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.hidden(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b9fac41-39ba-4800-bec1-f42deac3856f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify\n",
    "_ = torch.manual_seed(42)\n",
    "charlm = CharModel(4, 64, 81)\n",
    "logits = charlm(torch.tensor([[10, 9, 12, 1], [1, 2, 3, 4]]))\n",
    "torch.allclose(logits[0, -1], torch.tensor(-0.0521), atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7192cd-6e9a-4cb1-8840-ca5973658f0d",
   "metadata": {},
   "source": [
    "Next, we will train a model with n=8, i.e. the model will try to predict the 9th character based on the 8 preceding characters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1efb5e1f-1f4c-4d1f-aee1-1f1857f98aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n",
      "There are 1328215 datapoints and 81 unique characters in the dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12:41:39 Training starts\n",
      "12:42:03 End of epoch 1 , loss= 2.127134084701538\n",
      "12:42:27 End of epoch 2 , loss= 2.2263357639312744\n",
      "12:42:52 End of epoch 3 , loss= 1.591212272644043\n",
      "12:43:15 End of epoch 4 , loss= 1.8571747541427612\n",
      "12:43:39 End of epoch 5 , loss= 1.6835119724273682\n",
      "12:44:02 End of epoch 6 , loss= 2.320404052734375\n",
      "12:44:26 End of epoch 7 , loss= 1.7896932363510132\n",
      "12:44:49 End of epoch 8 , loss= 2.187608480453491\n",
      "12:45:13 End of epoch 9 , loss= 1.923316240310669\n",
      "12:45:36 End of epoch 10 , loss= 1.1429024934768677\n",
      "12:45:59 End of epoch 11 , loss= 1.5131518840789795\n",
      "12:46:22 End of epoch 12 , loss= 1.7750986814498901\n",
      "12:46:46 End of epoch 13 , loss= 1.6273977756500244\n",
      "12:47:09 End of epoch 14 , loss= 1.4926913976669312\n",
      "12:47:33 End of epoch 15 , loss= 1.3606318235397339\n",
      "12:47:57 End of epoch 16 , loss= 1.9494916200637817\n",
      "12:48:20 End of epoch 17 , loss= 1.4308462142944336\n",
      "12:48:45 End of epoch 18 , loss= 1.0586464405059814\n",
      "12:49:09 End of epoch 19 , loss= 1.8919532299041748\n",
      "12:49:34 End of epoch 20 , loss= 1.4960280656814575\n",
      "12:49:59 End of epoch 21 , loss= 1.4142539501190186\n",
      "12:50:26 End of epoch 22 , loss= 1.2756656408309937\n",
      "12:50:52 End of epoch 23 , loss= 2.083681344985962\n",
      "12:51:19 End of epoch 24 , loss= 1.6687530279159546\n",
      "12:51:46 End of epoch 25 , loss= 1.4421794414520264\n",
      "12:52:11 End of epoch 26 , loss= 1.6652085781097412\n",
      "12:52:36 End of epoch 27 , loss= 1.2162929773330688\n",
      "12:53:00 End of epoch 28 , loss= 1.9062138795852661\n",
      "12:53:24 End of epoch 29 , loss= 1.0301984548568726\n",
      "12:53:49 End of epoch 30 , loss= 1.3999106884002686\n",
      "12:54:13 End of epoch 31 , loss= 1.5468111038208008\n",
      "12:54:38 End of epoch 32 , loss= 1.7898441553115845\n",
      "12:55:01 End of epoch 33 , loss= 1.404665231704712\n",
      "12:55:25 End of epoch 34 , loss= 0.9571993350982666\n",
      "12:55:49 End of epoch 35 , loss= 1.4556477069854736\n",
      "12:56:13 End of epoch 36 , loss= 0.950017511844635\n",
      "12:56:36 End of epoch 37 , loss= 1.557043433189392\n",
      "12:57:00 End of epoch 38 , loss= 1.3630620241165161\n",
      "12:57:24 End of epoch 39 , loss= 1.6728172302246094\n",
      "12:57:50 End of epoch 40 , loss= 1.3224931955337524\n",
      "12:58:14 End of epoch 41 , loss= 1.2343535423278809\n",
      "12:58:38 End of epoch 42 , loss= 0.9702923893928528\n",
      "12:59:02 End of epoch 43 , loss= 1.3455233573913574\n",
      "12:59:25 End of epoch 44 , loss= 1.4001150131225586\n",
      "12:59:49 End of epoch 45 , loss= 1.1863478422164917\n",
      "13:00:13 End of epoch 46 , loss= 1.3218992948532104\n",
      "13:00:37 End of epoch 47 , loss= 1.044755220413208\n",
      "13:01:01 End of epoch 48 , loss= 1.311091423034668\n",
      "13:01:25 End of epoch 49 , loss= 1.6605944633483887\n",
      "13:01:49 End of epoch 50 , loss= 1.298880934715271\n"
     ]
    }
   ],
   "source": [
    "# Choose 'Run all cells' in the 'Run' menu to run this cell.\n",
    "_ = torch.manual_seed(21)\n",
    "\n",
    "# ===================== Hyperparameters ================== #\n",
    "\n",
    "n = 8\n",
    "batch_size = 64\n",
    "hidden_size = 64\n",
    "learning_rate = 0.00001\n",
    "number_of_epochs = 50\n",
    "\n",
    "# ======================= Training ======================= #\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"Running on\", device)\n",
    "\n",
    "training_dataset = CharDataset(data_path, n)\n",
    "print(\"There are\", len(training_dataset), \"datapoints and\",\n",
    "       len(id_to_char), \"unique characters in the dataset\")\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    training_dataset, batch_size=batch_size, collate_fn=char_collate_fn, shuffle=True)\n",
    "charlm = CharModel(n, hidden_size, len(char_to_id)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "charlm_optimizer = optim.Adam(charlm.parameters(), lr=learning_rate)\n",
    "\n",
    "charlm.train()\n",
    "print(datetime.now().strftime(\"%X\"), \"Training starts\")\n",
    "for epoch in range(number_of_epochs):\n",
    "    for input_tensor, label in training_loader:\n",
    "        input_tensor, label = input_tensor.to(device), label.to(device)\n",
    "        charlm_optimizer.zero_grad()\n",
    "        logits = charlm(input_tensor).to(device)\n",
    "        loss = criterion(logits.squeeze(1), label)\n",
    "        loss.backward()\n",
    "        charlm_optimizer.step()\n",
    "    print(datetime.now().strftime(\"%X\"), \"End of epoch\",\n",
    "          epoch+1, \", loss=\", loss.detach().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02427a50-617e-476b-b3b3-ddfbac543286",
   "metadata": {},
   "source": [
    "Check how well the model works by entering a string and letting the model generate the continuation of that string.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c5a3903-5bc9-4a8e-8299-fcb54f037f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry and Hermione kissed the corridor and the corridor and the corridor and the corridor and the corridor and the corridor and the corridor and the corridor and the corridor and the corridor and the corridor and the corridor\n"
     ]
    }
   ],
   "source": [
    "charlm.eval()\n",
    "while True:\n",
    "    start = input(\">\")\n",
    "    if start.strip() == 'quit':\n",
    "        break\n",
    "    # Add spaces in case the start string is too short\n",
    "    start = ' '*(n-len(start)) + start\n",
    "    # Ignore everything but the last n characters of the start string\n",
    "    ids = [char_to_id[c] for c in start][-n:]\n",
    "    # Generate 200 characters starting from the start string\n",
    "    try:\n",
    "        print(start, end='')\n",
    "        for _ in range(200):\n",
    "            input_tensor = torch.tensor(ids).unsqueeze(0).to(device)\n",
    "            logits = charlm(input_tensor).squeeze().to(device)\n",
    "            _, new_character_tensor = logits.topk(1)\n",
    "            new_character_id = new_character_tensor.detach().item()\n",
    "            print(id_to_char[new_character_id], end='')\n",
    "            ids.pop(0)\n",
    "            ids.append(new_character_id)\n",
    "        print()\n",
    "    except KeyError:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
