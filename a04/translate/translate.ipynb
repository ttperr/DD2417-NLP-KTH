{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25306ba6-bbef-4762-b03c-09204d3c74ac",
   "metadata": {},
   "source": [
    "# Assignment 4, task 2\n",
    "\n",
    "Recurrent neural networks, particularly Gated Recurrent Units, were developed with a translation task in mind. We will now do a small English-to-Swedish translation experiment by training on a corpus from Tatoeba (https://tatoeba.org/en/).\n",
    "\n",
    "To this end, we are going to use an Encoder-Decoder architecture. The encoder is a recurrent neural network with GRU cells, that encodes the English input sentence. The Encoder can be either uni-directional or bi-directional.\n",
    "\n",
    "After the encoder has processed the English sentence, the decoder then takes over and generates the Swedish sentence, starting from the final hidden state of the encoder (or the concatenation of the two final hidden states, in the case of a bi-directional encoder). If an attention mechanism is used, the decoder will access all the hidden states of the encoder when deciding which word to output next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ccda08cf-563d-46f7-8eb2-9c20f3c43a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run this cell\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import random\n",
    "import pickle\n",
    "import codecs\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from terminaltables import AsciiTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0146c189-c61f-481f-9e02-970e95ac68f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mappings between symbols and integers, and vice versa.\n",
    "# They are global for all datasets.\n",
    "source_w2i = {}\n",
    "source_i2w = []\n",
    "target_w2i = {}\n",
    "target_i2w = []\n",
    "\n",
    "# The padding symbol will be used to ensure that all tensors in a batch\n",
    "# have equal length.\n",
    "PADDING_SYMBOL = ' '\n",
    "source_w2i[PADDING_SYMBOL] = 0\n",
    "source_i2w.append(PADDING_SYMBOL)\n",
    "target_w2i[PADDING_SYMBOL] = 0\n",
    "target_i2w.append(PADDING_SYMBOL)\n",
    "\n",
    "START_SYMBOL = '<START>'\n",
    "END_SYMBOL = '<END>'\n",
    "UNK_SYMBOL = '<UNK>'\n",
    "source_w2i[START_SYMBOL] = 1\n",
    "source_i2w.append(START_SYMBOL)\n",
    "target_w2i[START_SYMBOL] = 1\n",
    "target_i2w.append(START_SYMBOL)\n",
    "source_w2i[END_SYMBOL] = 2\n",
    "source_i2w.append(END_SYMBOL)\n",
    "target_w2i[END_SYMBOL] = 2\n",
    "target_i2w.append(END_SYMBOL)\n",
    "source_w2i[UNK_SYMBOL] = 3\n",
    "source_i2w.append(UNK_SYMBOL)\n",
    "target_w2i[UNK_SYMBOL] = 3\n",
    "target_i2w.append(UNK_SYMBOL)\n",
    "\n",
    "# Max number of words to be predicted if <END> symbol is not reached\n",
    "MAX_PREDICTIONS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3c52eda7-56cc-49c9-a34c-36985fb666de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(embedding_file):\n",
    "    \"\"\"\n",
    "    Reads pre-made embeddings from a file\n",
    "    \"\"\"\n",
    "    N = len(source_w2i)\n",
    "    embeddings = [0]*N\n",
    "    with codecs.open(embedding_file, 'r', 'utf-8') as f:\n",
    "        for line in f:\n",
    "            data = line.split()\n",
    "            word = data[0].lower()\n",
    "            if word not in source_w2i:\n",
    "                source_w2i[word] = N\n",
    "                source_i2w.append(word)\n",
    "                N += 1\n",
    "                embeddings.append(0)\n",
    "            vec = [float(x) for x in data[1:]]\n",
    "            D = len(vec)\n",
    "            embeddings[source_w2i[word]] = vec\n",
    "    # Add a '0' embedding for the padding symbol\n",
    "    embeddings[0] = [0]*D\n",
    "    # Check if there are words that did not have a ready-made Glove embedding\n",
    "    # For these words, add a random vector\n",
    "    for word in source_w2i:\n",
    "        index = source_w2i[word]\n",
    "        if embeddings[index] == 0:\n",
    "            embeddings[index] = (np.random.random(D)-0.5).tolist()\n",
    "    return D, embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45ff9d9f-fb01-46cc-83c9-466a0bbb3acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset with source sentences and their respective translations\n",
    "    into the target language.\n",
    "\n",
    "    Each sentence is represented as a list of word IDs. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename, record_symbols=True):\n",
    "        try:\n",
    "            nltk.word_tokenize(\"hi there.\")\n",
    "        except LookupError:\n",
    "            nltk.download('punkt')\n",
    "        self.source_list = []\n",
    "        self.target_list = []\n",
    "        # Read the datafile\n",
    "        with codecs.open(filename, 'r', 'utf-8') as f:\n",
    "            lines = f.read().split('\\n')\n",
    "            for line in lines:\n",
    "                if '\\t' not in line:\n",
    "                    continue\n",
    "                s, t = line.split('\\t')\n",
    "                source_sentence = []\n",
    "                for w in nltk.word_tokenize(s):\n",
    "                    if w not in source_i2w and record_symbols:\n",
    "                        source_w2i[w] = len(source_i2w)\n",
    "                        source_i2w.append(w)\n",
    "                    source_sentence.append(\n",
    "                        source_w2i.get(w, source_w2i[UNK_SYMBOL]))\n",
    "                source_sentence.append(source_w2i[END_SYMBOL])\n",
    "                self.source_list.append(source_sentence)\n",
    "                target_sentence = []\n",
    "                for w in nltk.word_tokenize(t):\n",
    "                    if w not in target_i2w and record_symbols:\n",
    "                        target_w2i[w] = len(target_i2w)\n",
    "                        target_i2w.append(w)\n",
    "                    target_sentence.append(\n",
    "                        target_w2i.get(w, target_w2i[UNK_SYMBOL]))\n",
    "                target_sentence.append(target_w2i[END_SYMBOL])\n",
    "                self.target_list.append(target_sentence)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.source_list[idx], self.target_list[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "920bdc47-f44b-4180-98f4-2ea1b18892c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell. The function below will take care of the case of\n",
    "# sequences of unequal lengths.\n",
    "\n",
    "def pad_sequence(batch, pad_source=source_w2i[PADDING_SYMBOL], pad_target=target_w2i[PADDING_SYMBOL]):\n",
    "    source, target = zip(*batch)\n",
    "    max_source_len = max(map(len, source))\n",
    "    max_target_len = max(map(len, target))\n",
    "    padded_source = [[b[i] if i < len(b) else pad_source for i in range(\n",
    "        max_source_len)] for b in source]\n",
    "    padded_target = [[l[i] if i < len(l) else pad_target for i in range(\n",
    "        max_target_len)] for l in target]\n",
    "    return padded_source, padded_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba3edb-3ff2-4071-a24a-97e52a55a921",
   "metadata": {},
   "source": [
    "Here is the implementation of the encoder. For task 2(a), you will need to fill a part of the code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9d526e15-f4a3-4d73-861a-fc878773d0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Encoder ==================== #\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Encodes a batch of source sentences. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, no_of_input_symbols, embeddings=None, embedding_size=16, hidden_size=25,\n",
    "                 encoder_bidirectional=False, device='cpu', use_gru=False, tune_embeddings=False):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.is_bidirectional = encoder_bidirectional\n",
    "        self.embedding = nn.Embedding(no_of_input_symbols, embedding_size)\n",
    "        if embeddings != None:\n",
    "            self.embedding.weight = nn.Parameter(torch.tensor(\n",
    "                embeddings, dtype=torch.float), requires_grad=tune_embeddings)\n",
    "        if use_gru:\n",
    "            self.rnn = nn.GRU(embedding_size, hidden_size,\n",
    "                              batch_first=True, bidirectional=self.is_bidirectional)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size,\n",
    "                              batch_first=True, bidirectional=self.is_bidirectional)\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def set_embeddings(self, embeddings):\n",
    "        self.embedding.weight = torch.tensor(embeddings, dtype=torch.float)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x is a list of lists of size (batch_size,max_seq_length)\n",
    "        Each inner list contains word IDs and represents one sentence.\n",
    "        The whole list-of-lists represents a batch of sentences.\n",
    "\n",
    "        Returns:\n",
    "        the output from the encoder RNN: a pair of two tensors, one containing all hidden states, and one \n",
    "        containing the last hidden state (see https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)\n",
    "        \"\"\"\n",
    "\n",
    "        x_tensor = torch.tensor(x).to(self.device)\n",
    "\n",
    "        # FOR TASK (a), REPLACE THE FOLLOWING LINE WITH YOUR CODE\n",
    "        embedded = self.embedding(x_tensor)\n",
    "        return self.rnn(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40c6219-14aa-483b-ad15-46db4e6705af",
   "metadata": {},
   "source": [
    "Here is the decoder. For tasks (b) and (c), fill in the missing code in the 'forward' function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f9962c2c-b5d6-49fc-b2e4-f9e20a914fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================== Decoder ==================== #\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, no_of_output_symbols, embedding_size=16, hidden_size=25, use_attention=True,\n",
    "                 display_attention=False, device='cpu', use_gru=False):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(no_of_output_symbols, embedding_size)\n",
    "        self.no_of_output_symbols = no_of_output_symbols\n",
    "        # shouldn't W be 2*hidden_size\n",
    "        self.W = nn.Parameter(torch.rand(hidden_size, hidden_size)-0.5)\n",
    "        self.U = nn.Parameter(torch.rand(hidden_size, hidden_size)-0.5)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size, 1)-0.5)\n",
    "        self.use_attention = use_attention\n",
    "        self.display_attention = display_attention\n",
    "        if use_gru:\n",
    "            self.rnn = nn.GRU(embedding_size, hidden_size, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(embedding_size, hidden_size, batch_first=True)\n",
    "        self.output = nn.Linear(hidden_size, no_of_output_symbols)\n",
    "        self.device = device\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, inp, hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "        'input' is a list of length batch_size, containing the current word\n",
    "        of each sentence in the batch\n",
    "\n",
    "        'hidden' is a tensor containing the last hidden state of the decoder, \n",
    "        for each sequence in the batch\n",
    "        hidden.shape = (1, batch_size, hidden_size)\n",
    "\n",
    "        'encoder_outputs' is a tensor containing all hidden states from the\n",
    "        encoder (used in problem c)\n",
    "        encoder_outputs.shape = (batch_size, max_seq_length, hidden_size)\n",
    "\n",
    "        Note that 'max_seq_length' above refers to the max_seq_length\n",
    "        of the encoded sequence (not the decoded sequence).\n",
    "\n",
    "        Returns:\n",
    "        If use_attention and display_attention are both True (task (c)), return a triple\n",
    "        (logits for the predicted next word, hidden state, attention weights alpha)\n",
    "\n",
    "        Otherwise (task (b)), return a pair\n",
    "        (logits for the predicted next word, hidden state).\n",
    "        \"\"\"\n",
    "        inp_tensor = torch.tensor(inp).to(self.device)\n",
    "\n",
    "        # FOR (b) and (c) REPLACE THE FOLLOWING LINE WITH YOUR CODE\n",
    "        embedded = self.embedding(inp_tensor).unsqueeze(1)\n",
    "\n",
    "        if self.use_attention and self.display_attention:\n",
    "            # FOR TASK (c), REPLACE THE FOLLOWING LINES WITH YOUR CODE\n",
    "            # Calculate attention weights\n",
    "            alpha = torch.matmul(torch.tanh(torch.matmul(hidden.permute(1, 0, 2), self.W) +\n",
    "                                            torch.matmul(encoder_outputs, self.U)), self.v)\n",
    "            alpha = F.softmax(alpha, dim=1)\n",
    "            # Calculate context vector\n",
    "            context = torch.bmm(alpha.permute(0, 2, 1), encoder_outputs)\n",
    "            context = context.permute(1, 0, 2)\n",
    "            output, hidden = self.rnn(embedded, context)\n",
    "            return self.output(output), hidden, alpha\n",
    "\n",
    "        else:\n",
    "            output, hidden = self.rnn(embedded, hidden)\n",
    "            return self.output(output), hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7835bb5-a9db-4cb6-b044-1a9b0581ff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will be used for evaluation of both the dev set (during training)\n",
    "# and the test set (after training is finished).\n",
    "def evaluate(ds, encoder, decoder):\n",
    "    confusion = [[0 for a in target_i2w] for b in target_i2w]\n",
    "    correct_sentences, incorrect_sentences = 0, 0\n",
    "    for x, y in ds:\n",
    "        predicted_sentence = []\n",
    "        outputs, hidden = encoder([x])\n",
    "        if encoder.is_bidirectional:\n",
    "            hidden = hidden.permute((1, 0, 2)).reshape(1, -1).unsqueeze(0)\n",
    "        predicted_symbol = target_w2i[START_SYMBOL]\n",
    "        for correct in y:\n",
    "            predictions, hidden = decoder([predicted_symbol], hidden, outputs)\n",
    "            _, predicted_tensor = predictions.topk(1)\n",
    "            predicted_symbol = predicted_tensor.detach().item()\n",
    "            confusion[int(predicted_symbol)][int(correct)] += 1\n",
    "            predicted_sentence.append(predicted_symbol)\n",
    "        if predicted_sentence == y:\n",
    "            correct_sentences += 1\n",
    "        else:\n",
    "            incorrect_sentences += 1\n",
    "    correct_symbols = sum([confusion[i][i] for i in range(len(confusion))])\n",
    "    all_symbols = torch.tensor(confusion).sum().item()\n",
    "\n",
    "    # Construct a neat confusion matrix\n",
    "    for i in range(len(confusion)):\n",
    "        confusion[i].insert(0, target_i2w[i])\n",
    "    first_row = [\"Predicted/Real\"]\n",
    "    first_row.extend(target_i2w)\n",
    "    confusion.insert(0, first_row)\n",
    "    # t = AsciiTable( confusion )\n",
    "\n",
    "    # print( t.table )\n",
    "    print(\"Correctly predicted words    : \", correct_symbols)\n",
    "    print(\"Incorrectly predicted words  : \", all_symbols-correct_symbols)\n",
    "    print(\"Correctly predicted sentences  : \", correct_sentences)\n",
    "    print(\"Incorrectly predicted sentences: \", incorrect_sentences)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7b6466f4-b193-4b83-b88b-f492b4429e02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: NVIDIA H100 80GB HBM3 MIG 1g.10gb\n",
      "\n",
      "Number of source words:  8883\n",
      "Number of target words:  12861\n",
      "Number of training sentences:  33454\n",
      "\n",
      "15:31:53 Starting training.\n",
      "15:32:02 Epoch 0 loss: 23.64399528503418\n",
      "Evaluating on the dev data...\n",
      "Correctly predicted words    :  358\n",
      "Incorrectly predicted words  :  2296\n",
      "Correctly predicted sentences  :  0\n",
      "Incorrectly predicted sentences:  370\n",
      "\n",
      "15:32:31 Epoch 1 loss: 15.650080680847168\n",
      "15:32:40 Epoch 2 loss: 14.015079498291016\n",
      "15:32:49 Epoch 3 loss: 12.933985710144043\n",
      "15:32:58 Epoch 4 loss: 12.133914947509766\n",
      "15:33:08 Epoch 5 loss: 11.500115394592285\n",
      "15:33:18 Epoch 6 loss: 10.970999717712402\n",
      "15:33:27 Epoch 7 loss: 10.514049530029297\n",
      "15:33:36 Epoch 8 loss: 10.108987808227539\n",
      "15:33:45 Epoch 9 loss: 9.744508743286133\n",
      "15:33:55 Epoch 10 loss: 9.411721229553223\n",
      "Evaluating on the dev data...\n",
      "Correctly predicted words    :  808\n",
      "Incorrectly predicted words  :  1846\n",
      "Correctly predicted sentences  :  0\n",
      "Incorrectly predicted sentences:  370\n",
      "\n",
      "15:34:22 Epoch 11 loss: 9.106687545776367\n",
      "15:34:32 Epoch 12 loss: 8.825261116027832\n",
      "15:34:41 Epoch 13 loss: 8.562552452087402\n",
      "15:34:51 Epoch 14 loss: 8.319902420043945\n",
      "15:35:00 Epoch 15 loss: 8.085098266601562\n",
      "15:35:10 Epoch 16 loss: 7.861852645874023\n",
      "15:35:19 Epoch 17 loss: 7.653925895690918\n",
      "15:35:28 Epoch 18 loss: 7.45553731918335\n",
      "15:35:38 Epoch 19 loss: 7.274513244628906\n",
      "15:35:47 Epoch 20 loss: 7.103175163269043\n",
      "Evaluating on the dev data...\n",
      "Correctly predicted words    :  906\n",
      "Incorrectly predicted words  :  1748\n",
      "Correctly predicted sentences  :  6\n",
      "Incorrectly predicted sentences:  364\n",
      "\n",
      "15:36:15 Epoch 21 loss: 6.943872451782227\n",
      "15:36:25 Epoch 22 loss: 6.800534248352051\n",
      "15:36:34 Epoch 23 loss: 6.672881126403809\n",
      "15:36:43 Epoch 24 loss: 6.543919086456299\n",
      "15:36:52 Epoch 25 loss: 6.4059038162231445\n",
      "15:37:01 Epoch 26 loss: 6.274131774902344\n",
      "15:37:11 Epoch 27 loss: 6.148810386657715\n",
      "15:37:20 Epoch 28 loss: 6.035605430603027\n",
      "15:37:29 Epoch 29 loss: 5.930665969848633\n",
      "15:37:38 Epoch 30 loss: 5.830346584320068\n",
      "Evaluating on the dev data...\n",
      "Correctly predicted words    :  898\n",
      "Incorrectly predicted words  :  1756\n",
      "Correctly predicted sentences  :  5\n",
      "Incorrectly predicted sentences:  365\n",
      "\n",
      "15:38:07 Epoch 31 loss: 5.733646392822266\n",
      "15:38:16 Epoch 32 loss: 5.647104263305664\n",
      "15:38:25 Epoch 33 loss: 5.570517539978027\n",
      "15:38:34 Epoch 34 loss: 5.495300769805908\n",
      "15:38:44 Epoch 35 loss: 5.4264116287231445\n",
      "15:38:54 Epoch 36 loss: 5.353183269500732\n",
      "15:39:03 Epoch 37 loss: 5.286969184875488\n",
      "15:39:12 Epoch 38 loss: 5.201371192932129\n",
      "15:39:22 Epoch 39 loss: 5.102877616882324\n",
      "15:39:32 Epoch 40 loss: 5.029393672943115\n",
      "Evaluating on the dev data...\n",
      "Correctly predicted words    :  926\n",
      "Incorrectly predicted words  :  1728\n",
      "Correctly predicted sentences  :  6\n",
      "Incorrectly predicted sentences:  364\n",
      "\n",
      "15:39:59 Epoch 41 loss: 4.970853328704834\n",
      "15:40:09 Epoch 42 loss: 4.91855525970459\n",
      "15:40:18 Epoch 43 loss: 4.875067710876465\n",
      "15:40:28 Epoch 44 loss: 4.84111213684082\n",
      "15:40:37 Epoch 45 loss: 4.771240711212158\n",
      "15:40:46 Epoch 46 loss: 4.711551189422607\n",
      "15:40:56 Epoch 47 loss: 4.6632771492004395\n",
      "15:41:05 Epoch 48 loss: 4.619119644165039\n",
      "15:41:14 Epoch 49 loss: 4.564444065093994\n",
      "Evaluating on the test data...\n",
      "Number of test sentences:  3346\n",
      "\n",
      "Correctly predicted words    :  8099\n",
      "Incorrectly predicted words  :  17158\n",
      "Correctly predicted sentences  :  79\n",
      "Incorrectly predicted sentences:  3267\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use 'Run all cells' to do the training.\n",
    "\n",
    "# ================ Hyper-parameters ================ #\n",
    "\n",
    "use_attention = True\n",
    "use_gru = True         # Use Gated Recurrent Units (rather than plain RNNs)\n",
    "bidirectional = True   # Use a bidirectional encoder\n",
    "use_embeddings = True      # Use pre-loaded Glove embeddings\n",
    "tune_embeddings = True  # Fine-tune the Glove embeddings\n",
    "batch_size = 64\n",
    "hidden_size = 25       # Number of dimensions in the hidden state\n",
    "learning_rate = 0.001\n",
    "epochs = 50            # We will train for this many epochs\n",
    "save = True           # Do not save the model\n",
    "\n",
    "# ====================== Data ===================== #\n",
    "\n",
    "training_file = '/datasets/dd2417/eng-swe-train.txt'\n",
    "test_file = '/datasets/dd2417/eng-swe-test.txt'\n",
    "dev_file = '/datasets/dd2417/eng-swe-dev.txt'\n",
    "\n",
    "# ==================== Training ==================== #\n",
    "# Reproducibility\n",
    "# Read a bit more here -- https://pytorch.org/docs/stable/notes/randomness.html\n",
    "random.seed(5719)\n",
    "np.random.seed(5719)\n",
    "# torch.manual_seed(5719)\n",
    "# torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Can we run on GPU?\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Current device: {}\".format(torch.cuda.get_device_name(0)))\n",
    "else:\n",
    "    print('Running on CPU')\n",
    "print()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Read datasets\n",
    "training_dataset = TranslationDataset(training_file)\n",
    "dev_dataset = TranslationDataset(dev_file, record_symbols=False)\n",
    "\n",
    "print(\"Number of source words: \", len(source_i2w))\n",
    "print(\"Number of target words: \", len(target_i2w))\n",
    "print(\"Number of training sentences: \", len(training_dataset))\n",
    "print()\n",
    "\n",
    "# If we have pre-computed word embeddings, then make sure these are used\n",
    "if use_embeddings:\n",
    "    embedding_size, embeddings = load_glove_embeddings(\n",
    "        '/datasets/dd2417/glove.6B.50d.txt')\n",
    "else:\n",
    "    embedding_size = args.hidden_size\n",
    "    embeddings = None\n",
    "\n",
    "training_loader = DataLoader(\n",
    "    training_dataset, batch_size=batch_size, collate_fn=pad_sequence)\n",
    "dev_loader = DataLoader(\n",
    "    dev_dataset, batch_size=batch_size, collate_fn=pad_sequence)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "encoder = EncoderRNN(\n",
    "    len(source_i2w),\n",
    "    embeddings=embeddings,\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size,\n",
    "    encoder_bidirectional=bidirectional,\n",
    "    tune_embeddings=tune_embeddings,\n",
    "    use_gru=use_gru,\n",
    "    device=device\n",
    ")\n",
    "decoder = DecoderRNN(\n",
    "    len(target_i2w),\n",
    "    embedding_size=embedding_size,\n",
    "    hidden_size=hidden_size*(bidirectional+1),\n",
    "    use_attention=use_attention,\n",
    "    use_gru=use_gru,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "print(datetime.now().strftime(\"%H:%M:%S\"), \"Starting training.\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    # tqdm(training_loader, desc=\"Epoch {}\".format(epoch + 1)):\n",
    "    for source, target in training_loader:\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "        loss = 0\n",
    "        # hidden is (D * num_layers, B, H)\n",
    "        outputs, hidden = encoder(source)\n",
    "        if bidirectional:\n",
    "            # (2, B, H) -> (B, 2 * H) -> (1, B, 2 * H)\n",
    "            hidden = torch.cat(\n",
    "                [hidden[0, :, :], hidden[1, :, :]], dim=1).unsqueeze(0)\n",
    "\n",
    "        # The probability of doing teacher forcing will decrease\n",
    "        # from 1 to 0 over the range of epochs. This could be implemented\n",
    "        # like this:\n",
    "        # teacher_forcing_ratio = 1- epoch/args.epochs\n",
    "        # But, for now we will always use teacher forcing\n",
    "        teacher_forcing_ratio = 1\n",
    "\n",
    "        # The input to the decoder in the first time step will be\n",
    "        # the boundary symbol, regardless if we are using teacher\n",
    "        # forcing or not.\n",
    "        idx = [target_w2i[START_SYMBOL] for sublist in target]\n",
    "        predicted_symbol = [target_w2i[START_SYMBOL] for sublist in target]\n",
    "\n",
    "        target_length = len(target[0])\n",
    "        for i in range(target_length):\n",
    "            use_teacher_forcing = (random.random() < teacher_forcing_ratio)\n",
    "            if use_teacher_forcing:\n",
    "                predictions, hidden = decoder(idx, hidden, outputs)\n",
    "            else:\n",
    "                # Here we input the previous prediction rather than the\n",
    "                # correct symbol.\n",
    "                predictions, hidden = decoder(\n",
    "                    predicted_symbol, hidden, outputs)\n",
    "            _, predicted_tensor = predictions.topk(1)\n",
    "            predicted_symbol = predicted_tensor.squeeze().tolist()\n",
    "\n",
    "            # The targets will be the ith symbol of all the target\n",
    "            # strings. They will also be used as inputs for the next\n",
    "            # time step if we use teacher forcing.\n",
    "            idx = [sublist[i] for sublist in target]\n",
    "            loss += criterion(predictions.squeeze(),\n",
    "                              torch.tensor(idx).to(device))\n",
    "        loss /= (target_length * batch_size)\n",
    "        loss.backward()\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "        total_loss += loss\n",
    "    print(datetime.now().strftime(\"%H:%M:%S\"), \"Epoch\",\n",
    "          epoch, \"loss:\", total_loss.detach().item())\n",
    "    total_loss = 0\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Evaluating on the dev data...\")\n",
    "        evaluate(dev_dataset, encoder, decoder)\n",
    "\n",
    "# ==================== Save the model  ==================== #\n",
    "\n",
    "if (save):\n",
    "    dt = str(datetime.now()).replace(\n",
    "        ' ', '_').replace(':', '_').replace('.', '_')\n",
    "    newdir = 'model_' + dt\n",
    "    os.mkdir(newdir)\n",
    "    torch.save(encoder.state_dict(), os.path.join(newdir, 'encoder.model'))\n",
    "    torch.save(decoder.state_dict(), os.path.join(newdir, 'decoder.model'))\n",
    "    with open(os.path.join(newdir, 'source_w2i'), 'wb') as f:\n",
    "        pickle.dump(source_w2i, f)\n",
    "        f.close()\n",
    "    with open(os.path.join(newdir, 'source_i2w'), 'wb') as f:\n",
    "        pickle.dump(source_i2w, f)\n",
    "        f.close()\n",
    "    with open(os.path.join(newdir, 'target_w2i'), 'wb') as f:\n",
    "        pickle.dump(target_w2i, f)\n",
    "        f.close()\n",
    "    with open(os.path.join(newdir, 'target_i2w'), 'wb') as f:\n",
    "        pickle.dump(target_i2w, f)\n",
    "        f.close()\n",
    "\n",
    "    settings = {\n",
    "        'training_set': training_file,\n",
    "        'test_set': test_file,\n",
    "        'epochs': epochs,\n",
    "        'learning_rate': learning_rate,\n",
    "        'batch_size': batch_size,\n",
    "        'hidden_size': hidden_size,\n",
    "        'attention': use_attention,\n",
    "        'bidirectional': bidirectional,\n",
    "        'embedding_size': embedding_size,\n",
    "        'use_gru': use_gru,\n",
    "        'tune_embeddings': tune_embeddings\n",
    "    }\n",
    "    with open(os.path.join(newdir, 'settings.json'), 'w') as f:\n",
    "        json.dump(settings, f)\n",
    "\n",
    "# ==================== Evaluation ==================== #\n",
    "\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "print(\"Evaluating on the test data...\")\n",
    "\n",
    "test_dataset = TranslationDataset(test_file, record_symbols=False)\n",
    "print(\"Number of test sentences: \", len(test_dataset))\n",
    "print()\n",
    "\n",
    "evaluate(test_dataset, encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "607a61f1-fe2c-4806-9387-cc7811d14489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "det där projektet var borta till sjukhuset mig slut kom för sent till sjukhuset mig slut kom för sent till \n",
      "+---------------+------+------+-----------+------+-------+------+-----------+------+------+------+------+------+------+-----------+------+------+------+------+------+------+\n",
      "| Source/Result | det  | där  | projektet | var  | borta | till | sjukhuset | mig  | slut | kom  | för  | sent | till | sjukhuset | mig  | slut | kom  | för  | sent | till |\n",
      "+---------------+------+------+-----------+------+-------+------+-----------+------+------+------+------+------+------+-----------+------+------+------+------+------+------+\n",
      "| it            | 0.05 | 0.07 | 0.08      | 0.16 | 0.03  | 0.01 | 0.03      | 0.09 | 0.05 | 0.11 | 0.04 | 0.07 | 0.07 | 0.03      | 0.09 | 0.05 | 0.11 | 0.04 | 0.07 | 0.07 |\n",
      "| is            | 0.07 | 0.08 | 0.10      | 0.11 | 0.05  | 0.05 | 0.06      | 0.10 | 0.05 | 0.20 | 0.13 | 0.23 | 0.12 | 0.03      | 0.11 | 0.05 | 0.20 | 0.13 | 0.23 | 0.12 |\n",
      "| seven         | 0.11 | 0.22 | 0.22      | 0.21 | 0.19  | 0.09 | 0.25      | 0.16 | 0.19 | 0.19 | 0.16 | 0.11 | 0.09 | 0.21      | 0.15 | 0.19 | 0.19 | 0.16 | 0.11 | 0.09 |\n",
      "| o'clock       | 0.19 | 0.10 | 0.16      | 0.16 | 0.13  | 0.27 | 0.17      | 0.21 | 0.16 | 0.23 | 0.13 | 0.10 | 0.25 | 0.14      | 0.21 | 0.16 | 0.23 | 0.13 | 0.10 | 0.25 |\n",
      "| .             | 0.59 | 0.54 | 0.44      | 0.35 | 0.59  | 0.58 | 0.49      | 0.43 | 0.55 | 0.28 | 0.54 | 0.49 | 0.47 | 0.60      | 0.45 | 0.55 | 0.28 | 0.54 | 0.49 | 0.47 |\n",
      "+---------------+------+------+-----------+------+-------+------+-----------+------+------+------+------+------+------+-----------+------+------+------+------+------+------+\n",
      "nu måste vi måste vi måste vi måste vi måste vi måste vi måste vi måste vi måste vi måste \n",
      "+---------------+------+-------+------+-------+------+-------+------+-------+------+-------+------+-------+------+-------+------+-------+------+-------+------+-------+\n",
      "| Source/Result | nu   | måste | vi   | måste | vi   | måste | vi   | måste | vi   | måste | vi   | måste | vi   | måste | vi   | måste | vi   | måste | vi   | måste |\n",
      "+---------------+------+-------+------+-------+------+-------+------+-------+------+-------+------+-------+------+-------+------+-------+------+-------+------+-------+\n",
      "| i             | 0.01 | 0.02  | 0.03 | 0.02  | 0.02 | 0.02  | 0.02 | 0.02  | 0.02 | 0.02  | 0.02 | 0.02  | 0.02 | 0.02  | 0.02 | 0.02  | 0.02 | 0.02  | 0.02 | 0.02  |\n",
      "| should        | 0.01 | 0.05  | 0.10 | 0.04  | 0.04 | 0.04  | 0.04 | 0.04  | 0.04 | 0.04  | 0.04 | 0.04  | 0.04 | 0.04  | 0.04 | 0.04  | 0.04 | 0.04  | 0.04 | 0.04  |\n",
      "| go            | 0.03 | 0.13  | 0.11 | 0.06  | 0.08 | 0.05  | 0.08 | 0.05  | 0.08 | 0.05  | 0.08 | 0.05  | 0.08 | 0.05  | 0.08 | 0.05  | 0.08 | 0.05  | 0.08 | 0.05  |\n",
      "| to            | 0.05 | 0.11  | 0.14 | 0.14  | 0.09 | 0.14  | 0.09 | 0.14  | 0.09 | 0.14  | 0.09 | 0.14  | 0.09 | 0.14  | 0.09 | 0.14  | 0.09 | 0.14  | 0.09 | 0.14  |\n",
      "| bed           | 0.16 | 0.11  | 0.19 | 0.14  | 0.27 | 0.18  | 0.28 | 0.17  | 0.28 | 0.17  | 0.28 | 0.17  | 0.28 | 0.17  | 0.28 | 0.17  | 0.28 | 0.17  | 0.28 | 0.17  |\n",
      "| now           | 0.57 | 0.41  | 0.27 | 0.35  | 0.35 | 0.32  | 0.33 | 0.32  | 0.33 | 0.32  | 0.33 | 0.32  | 0.33 | 0.32  | 0.33 | 0.32  | 0.33 | 0.32  | 0.33 | 0.32  |\n",
      "| .             | 0.17 | 0.18  | 0.15 | 0.25  | 0.16 | 0.25  | 0.16 | 0.25  | 0.16 | 0.25  | 0.16 | 0.25  | 0.16 | 0.25  | 0.16 | 0.25  | 0.16 | 0.25  | 0.16 | 0.25  |\n",
      "+---------------+------+-------+------+-------+------+-------+------+-------+------+-------+------+-------+------+-------+------+-------+------+-------+------+-------+\n",
      "lycka som tycker att dividera tar de flesta vänner vill du tycker du liksom massage får lilla barn tycker att \n",
      "+---------------+-------+------+--------+------+----------+------+------+--------+--------+------+------+--------+------+--------+---------+------+-------+------+--------+------+\n",
      "| Source/Result | lycka | som  | tycker | att  | dividera | tar  | de   | flesta | vänner | vill | du   | tycker | du   | liksom | massage | får  | lilla | barn | tycker | att  |\n",
      "+---------------+-------+------+--------+------+----------+------+------+--------+--------+------+------+--------+------+--------+---------+------+-------+------+--------+------+\n",
      "| i             | 0.02  | 0.04 | 0.07   | 0.07 | 0.05     | 0.06 | 0.07 | 0.05   | 0.11   | 0.06 | 0.07 | 0.03   | 0.14 | 0.04   | 0.15    | 0.11 | 0.08  | 0.03 | 0.07   | 0.12 |\n",
      "| do            | 0.21  | 0.10 | 0.20   | 0.15 | 0.28     | 0.19 | 0.24 | 0.16   | 0.23   | 0.22 | 0.11 | 0.38   | 0.23 | 0.25   | 0.24    | 0.19 | 0.22  | 0.18 | 0.22   | 0.30 |\n",
      "| n't           | 0.14  | 0.11 | 0.18   | 0.13 | 0.13     | 0.23 | 0.24 | 0.09   | 0.14   | 0.13 | 0.15 | 0.17   | 0.20 | 0.12   | 0.19    | 0.17 | 0.20  | 0.14 | 0.16   | 0.13 |\n",
      "| like          | 0.16  | 0.10 | 0.12   | 0.14 | 0.15     | 0.07 | 0.12 | 0.14   | 0.08   | 0.06 | 0.17 | 0.10   | 0.08 | 0.16   | 0.08    | 0.13 | 0.11  | 0.13 | 0.09   | 0.10 |\n",
      "| reading       | 0.16  | 0.27 | 0.21   | 0.23 | 0.17     | 0.13 | 0.13 | 0.32   | 0.15   | 0.19 | 0.20 | 0.15   | 0.09 | 0.20   | 0.09    | 0.19 | 0.12  | 0.24 | 0.22   | 0.17 |\n",
      "| books         | 0.30  | 0.39 | 0.22   | 0.29 | 0.23     | 0.31 | 0.20 | 0.23   | 0.28   | 0.33 | 0.30 | 0.17   | 0.26 | 0.22   | 0.27    | 0.22 | 0.27  | 0.28 | 0.23   | 0.18 |\n",
      "+---------------+-------+------+--------+------+----------+------+------+--------+--------+------+------+--------+------+--------+---------+------+-------+------+--------+------+\n",
      "alla skulle finnas alla har alla skulle finnas alla har alla skulle finnas alla har alla skulle finnas alla har \n",
      "+---------------+------+--------+--------+------+------+------+--------+--------+------+------+------+--------+--------+------+------+------+--------+--------+------+------+\n",
      "| Source/Result | alla | skulle | finnas | alla | har  | alla | skulle | finnas | alla | har  | alla | skulle | finnas | alla | har  | alla | skulle | finnas | alla | har  |\n",
      "+---------------+------+--------+--------+------+------+------+--------+--------+------+------+------+--------+--------+------+------+------+--------+--------+------+------+\n",
      "| thanks        | 0.39 | 0.35   | 0.27   | 0.29 | 0.15 | 0.29 | 0.34   | 0.24   | 0.29 | 0.15 | 0.29 | 0.34   | 0.24   | 0.29 | 0.15 | 0.29 | 0.34   | 0.24   | 0.29 | 0.15 |\n",
      "| a             | 0.18 | 0.12   | 0.24   | 0.09 | 0.11 | 0.26 | 0.30   | 0.24   | 0.09 | 0.11 | 0.26 | 0.30   | 0.24   | 0.09 | 0.11 | 0.26 | 0.30   | 0.24   | 0.09 | 0.11 |\n",
      "| lot           | 0.42 | 0.53   | 0.50   | 0.61 | 0.74 | 0.45 | 0.36   | 0.52   | 0.62 | 0.74 | 0.45 | 0.36   | 0.52   | 0.62 | 0.74 | 0.45 | 0.36   | 0.52   | 0.62 | 0.74 |\n",
      "+---------------+------+--------+--------+------+------+------+--------+--------+------+------+------+--------+--------+------+------+------+--------+--------+------+------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3404/600815527.py:47: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3614.)\n",
      "  ap = torch.tensor(attention_probs).T\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mamma vill emily följde ” smith ! <END> \n",
      "+---------------+-------+------+-------+--------+------+-------+------+-------+\n",
      "| Source/Result | mamma | vill | emily | följde | ”    | smith | !    | <END> |\n",
      "+---------------+-------+------+-------+--------+------+-------+------+-------+\n",
      "| hello         | 1.00  | 1.00 | 1.00  | 1.00   | 1.00 | 1.00  | 1.00 | 1.00  |\n",
      "+---------------+-------+------+-------+--------+------+-------+------+-------+\n",
      "jag bestämmer ! <END> \n",
      "+---------------+------+-----------+------+-------+\n",
      "| Source/Result | jag  | bestämmer | !    | <END> |\n",
      "+---------------+------+-----------+------+-------+\n",
      "| hello         | 0.33 | 0.32      | 0.62 | 0.30  |\n",
      "| .             | 0.67 | 0.68      | 0.38 | 0.70  |\n",
      "+---------------+------+-----------+------+-------+\n"
     ]
    }
   ],
   "source": [
    "# ==================== User interaction ==================== #\n",
    "\n",
    "decoder.display_attention = True\n",
    "while (True):\n",
    "    text = input(\"> \")\n",
    "    if text == \"\":\n",
    "        continue\n",
    "    if text == \"exit\":\n",
    "        break\n",
    "    try:\n",
    "        source_sentence = [source_w2i[w] for w in nltk.word_tokenize(text)]\n",
    "    except KeyError:\n",
    "        print(\"Erroneous input string\")\n",
    "        continue\n",
    "    outputs, hidden = encoder([source_sentence])\n",
    "    if encoder.is_bidirectional:\n",
    "        hidden = hidden.permute((1, 0, 2)).reshape(1, -1).unsqueeze(0)\n",
    "\n",
    "    predicted_symbol = target_w2i[START_SYMBOL]\n",
    "    target_sentence = []\n",
    "    attention_probs = []\n",
    "    num_attempts = 0\n",
    "    while num_attempts < MAX_PREDICTIONS:\n",
    "        if use_attention:\n",
    "            predictions, hidden, alpha = decoder(\n",
    "                [predicted_symbol], hidden, outputs)\n",
    "            attention_probs.append(alpha.permute(\n",
    "                0, 2, 1).squeeze().detach().tolist())\n",
    "        else:\n",
    "            predictions, hidden = decoder([predicted_symbol], hidden, outputs)\n",
    "\n",
    "        _, predicted_tensor = predictions.topk(1)\n",
    "        predicted_symbol = predicted_tensor.detach().item()\n",
    "        target_sentence.append(predicted_symbol)\n",
    "\n",
    "        num_attempts += 1\n",
    "\n",
    "        if predicted_symbol == target_w2i[END_SYMBOL]:\n",
    "            break\n",
    "\n",
    "    for i in target_sentence:\n",
    "        print(target_i2w[i].encode('utf-8').decode(), end=' ')\n",
    "    print()\n",
    "\n",
    "    if use_attention:\n",
    "        # Construct the attention table\n",
    "        ap = torch.tensor(attention_probs).T\n",
    "        if len(ap.shape) == 1:\n",
    "            ap = ap.unsqueeze(0)\n",
    "        attention_probs = ap.tolist()\n",
    "\n",
    "        for i in range(len(attention_probs)):\n",
    "            for j in range(len(attention_probs[i])):\n",
    "                attention_probs[i][j] = \"{val:.2f}\".format(\n",
    "                    val=attention_probs[i][j])\n",
    "        for i in range(len(attention_probs)):\n",
    "            if i < len(text):\n",
    "                attention_probs[i].insert(0, source_i2w[source_sentence[i]])\n",
    "            else:\n",
    "                attention_probs[i].insert(0, ' ')\n",
    "        first_row = [\"Source/Result\"]\n",
    "        for w in target_sentence:\n",
    "            first_row.append(target_i2w[w])\n",
    "        attention_probs.insert(0, first_row)\n",
    "        t = AsciiTable(attention_probs)\n",
    "        print(t.table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
