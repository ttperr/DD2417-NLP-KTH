{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6fd6acb-a67a-46f7-8564-a0498fbf15f5",
   "metadata": {},
   "source": [
    "# Assignment 4, task 1\n",
    "\n",
    "In this task, we will have a final crack at the NER problem, using recurrent neural networks, or Gated Recurrent Units (GRUs) to be more exact.\n",
    "\n",
    "We want to consider both the context of the word (the surrounding words) and the contents of the word (the letters and other symbols that make up the actual word). Therefore we are using two bi-directional GRUs, one world-level GRU for the words in the sentence, and one character-level GRU for the letters and other symbols in a word.\n",
    "\n",
    "We will process one sentence at a time. Each hidden state vector in the word-level GRU represents that word in relation to the other words in the sentence, whereas the final state vector(s) in the character-level RNN represent morphological and typographical information about the word. We will concatenate these vectors to obtain a single information-rich representation of the word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a9103d5-cadf-4599-9c4b-c5ef0115b866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First run this cell\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "import string\n",
    "import codecs\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils import clip_grad_norm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31cb9985-b6df-40b0-8b56-777265c79de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to init mappings from characters to IDs and back again,\n",
    "# from words to IDs and back again, and from labels to IDs and back again\n",
    "\n",
    "UNKNOWN = '<UNK>'  # Unknown char or unknown word\n",
    "CHARS = [UNKNOWN, '’', '—'] + \\\n",
    "    list(string.punctuation) + list(string.ascii_letters) + list(string.digits)\n",
    "char_to_id = {c: i for i, c in enumerate(CHARS)}\n",
    "PADDING_WORD = '<PAD>'\n",
    "id_to_label = ['noname', 'name']\n",
    "\n",
    "\n",
    "def label_to_id(label):\n",
    "    return 0 if label == 'O' else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fa4c07-a542-4250-94e7-7583dc2a4583",
   "metadata": {},
   "source": [
    "We want to have a vector representation of the syntactic and semantic properties of words, and in order to avoid having to train these from scratch, we are going to re-use pre-trained Glove vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f1ca98e-065f-496e-8778-ee5fc43cdc19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(embedding_file,\n",
    "                          padding_word=PADDING_WORD,\n",
    "                          unknown_word=UNKNOWN):\n",
    "    \"\"\"\n",
    "    Reads Glove embeddings from a file.\n",
    "\n",
    "    Returns vector dimensionality, the word_to_id mapping (as a dict),\n",
    "    and the embeddings (as a list of lists).\n",
    "    \"\"\"\n",
    "    word_to_id = {}  # Dictionary to store word-to-ID mapping\n",
    "    word_to_id[padding_word] = 0\n",
    "    word_to_id[unknown_word] = 1\n",
    "    embeddings = []\n",
    "    with open(embedding_file, encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            data = line.split()\n",
    "            word = data[0]\n",
    "            vec = [float(x) for x in data[1:]]\n",
    "            embeddings.append(vec)\n",
    "            word_to_id[word] = len(word_to_id)\n",
    "    D = len(embeddings[0])\n",
    "\n",
    "    # <PAD> has an embedding of just zeros\n",
    "    embeddings.insert(word_to_id[padding_word], [0]*D)\n",
    "    # <UNK> has an embedding of just minus-ones\n",
    "    embeddings.insert(word_to_id[unknown_word], [-1]*D)\n",
    "\n",
    "    return D, word_to_id, embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb5db8-b6f8-42d9-82b6-fd65e05f5f34",
   "metadata": {},
   "source": [
    "We can now create our dataset. Each datapoint will consist of a sentence and its associated labels for each word in the sentence. The label is either 1 (a name) or 0 (not a name).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "767c1320-1489-4865-ada3-40e7276570db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A class loading NER dataset from a CSV file to be used as an input \n",
    "    to PyTorch DataLoader.\n",
    "\n",
    "    The CSV file has 4 fields: sentence number (only at the start of a new\n",
    "    sentence), word, POS tag (ignored), and label.\n",
    "\n",
    "    Datapoints are sentences + associated labels for each word. If the \n",
    "    words have not been seen before (i.e, they are not found in the \n",
    "    'word_to_id' dict), they will be mapped to the unknown word '<UNK>'.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filename, word_to_id):\n",
    "        reader = csv.reader(codecs.open(filename, encoding='ascii',\n",
    "                                        errors='ignore'), delimiter=',')\n",
    "\n",
    "        self.sentences = []\n",
    "        self.labels = []\n",
    "\n",
    "        sentence, labels = [], []\n",
    "        for row in reader:\n",
    "            if row:\n",
    "                if row[0].strip():  # A new sentence begins\n",
    "                    if sentence and labels:\n",
    "                        self.sentences.append(sentence)\n",
    "                        self.labels.append(labels)\n",
    "                    sentence = [row[1].strip()]\n",
    "                    labels = [label_to_id(row[3].strip())]\n",
    "                else:\n",
    "                    sentence.append(row[1].strip())\n",
    "                    labels.append(label_to_id(row[3].strip()))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b78440d-4a87-460f-862c-7b5b769d3eeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding for the word 'good' looks like this:\n",
      "[-0.35586, 0.5213, -0.6107, -0.30131, 0.94862, -0.31539, -0.59831, 0.12188, -0.031943, 0.55695, -0.10621, 0.63399, -0.4734, -0.075895, 0.38247, 0.081569, 0.82214, 0.2222, -0.0083764, -0.7662, -0.56253, 0.61759, 0.20292, -0.048598, 0.87815, -1.6549, -0.77418, 0.15435, 0.94823, -0.3952, 3.7302, 0.82855, -0.14104, 0.016395, 0.21115, -0.036085, -0.15587, 0.86583, 0.26309, -0.71015, -0.03677, 0.0018282, -0.17704, 0.27032, 0.11026, 0.14133, -0.057322, 0.27207, 0.31305, 0.92771]\n",
      "\n",
      "There are 4542 data points in the test_set\n",
      "Data point 1600 is ['Isolated', 'grass', 'fires', 'continue', 'to', 'burn', 'in', 'the', 'southern', 'U.S.', 'states', 'of', 'Oklahoma', 'and', 'Texas', ',', 'but', 'they', 'have', 'weakened', 'since', 'killing', 'one', 'elderly', 'woman', 'and', 'scorching', 'dozens', 'of', 'homes', 'on', 'Tuesday', '.']\n",
      "It has the labels [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Let's check out some of these data structures\n",
    "dim, word_to_id, embeddings = load_glove_embeddings(\n",
    "    '/datasets/dd2417/glove.6B.50d.txt')\n",
    "print(\"The embedding for the word 'good' looks like this:\")\n",
    "print(embeddings[word_to_id['good']])\n",
    "print()\n",
    "\n",
    "# Read the data we are going to use for testing the model\n",
    "test_set = NERDataset('/datasets/dd2417/ner_test.csv', word_to_id)\n",
    "print(\"There are\", len(test_set), \"data points in the test_set\")\n",
    "dp = 1600\n",
    "sentence, labels = test_set[dp]\n",
    "print(\"Data point\", dp, \"is\", sentence)\n",
    "print(\"It has the labels\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c30114fe-2993-491c-9b5c-9a1870ff1892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell. The function below will take care of the case of\n",
    "# sequences of unequal lengths.\n",
    "\n",
    "def pad_sequence(batch, padding_word=PADDING_WORD, padding_label=0):\n",
    "    batch_data, batch_labels = zip(*batch)\n",
    "    max_len = max(map(len, batch_labels))\n",
    "    padded_data = [[b[i] if i < len(b) else padding_word for i in range(\n",
    "        max_len)] for b in batch_data]\n",
    "    padded_labels = [[l[i] if i < len(l) else padding_label for i in range(\n",
    "        max_len)] for l in batch_labels]\n",
    "    return padded_data, padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bd4b1b2e-e8db-4bec-b4ad-3bb1681efeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[1, 2, 3, '<PAD>'], [4, 5, '<PAD>', '<PAD>'], [6, 7, 8, 9]],\n",
       " [[0, 0, 1, 0], [1, 0, 0, 0], [0, 1, 1, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is how it works\n",
    "x = [([1, 2, 3], [0, 0, 1]), ([4, 5], [1, 0]), ([6, 7, 8, 9], [0, 1, 1, 0])]\n",
    "pad_sequence(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cbc550-82c8-4a5a-8e35-faf3c1682232",
   "metadata": {},
   "source": [
    "Here is the actual classifier, as a class extending the Pytorch 'nn.Module' class. Your task is to write the forward function (look for \"YOUR CODE HERE\" below).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bd6c9668-fbb3-4cfc-a173-7f8b590c2169",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, word_embeddings,  # Pre-trained word embeddings\n",
    "                 char_to_id,             # Mapping from chars to ids\n",
    "                 word_to_id,             # Mapping from words to ids\n",
    "                 char_emb_size=16,\n",
    "                 char_hidden_size=25,    # Hidden size of the character-level biRNN\n",
    "                 word_hidden_size=100,   # Hidden size of the word-level biRNN\n",
    "                 padding_word=PADDING_WORD,\n",
    "                 unknown_word=UNKNOWN,\n",
    "                 char_bidirectional=True,\n",
    "                 word_bidirectional=True,\n",
    "                 device='cpu'\n",
    "                 ):\n",
    "\n",
    "        super(NERClassifier, self).__init__()\n",
    "        self.padding_word = padding_word\n",
    "        self.unknown_word = unknown_word\n",
    "        self.char_to_id = char_to_id\n",
    "        self.word_to_id = word_to_id\n",
    "        self.char_emb_size = char_emb_size\n",
    "        self.char_hidden_size = char_hidden_size\n",
    "        self.word_hidden_size = word_hidden_size\n",
    "        self.char_bidirectional = char_bidirectional\n",
    "        self.word_bidirectional = word_bidirectional\n",
    "        self.device = device\n",
    "\n",
    "        # Create an embedding tensor for the words and import the Glove\n",
    "        # embeddings. The embeddings are frozen (i.e., they will not be\n",
    "        # updated during training).\n",
    "        vocabulary_size = len(word_embeddings)\n",
    "        self.word_emb_size = len(word_embeddings[0])\n",
    "\n",
    "        self.word_emb = nn.Embedding(vocabulary_size, self.word_emb_size)\n",
    "        self.word_emb.weight = nn.Parameter(torch.tensor(embeddings, dtype=torch.float),\n",
    "                                            requires_grad=False)\n",
    "\n",
    "        # Create an embedding tensor for character embeddings. These embeddings\n",
    "        # are learnt from scratch (i.e., they are not frozen).\n",
    "        if self.char_emb_size > 0:\n",
    "            self.char_emb = nn.Embedding(len(char_to_id), char_emb_size)\n",
    "            self.char_birnn = nn.GRU(\n",
    "                self.char_emb_size,\n",
    "                self.char_hidden_size,\n",
    "                bidirectional=char_bidirectional,\n",
    "                batch_first=True\n",
    "            )\n",
    "        else:\n",
    "            self.char_hidden_size = 0\n",
    "\n",
    "        multiplier = 2 if self.char_bidirectional else 1\n",
    "        self.word_birnn = nn.GRU(\n",
    "            self.word_emb_size + multiplier * self.char_hidden_size,  # input size\n",
    "            self.word_hidden_size,\n",
    "            bidirectional=word_bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Binary classification - 0 if not part of the name, 1 if a name\n",
    "        multiplier = 2 if self.word_bidirectional else 1\n",
    "        self.final_pred = nn.Linear(multiplier * self.word_hidden_size, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs a forward pass of a NER classifier\n",
    "        Takes as input a 2D list `x` of dimensionality (B, T),\n",
    "        where B is the batch size;\n",
    "              T is the max sentence length in the batch (shorter sentences\n",
    "              are already padded with the special token <PAD>)\n",
    "\n",
    "        Returns logits, i.e. the output of the last linear layer before applying softmax.\n",
    "\n",
    "        :param      x:    A batch of sentences\n",
    "        :type       x:    list of strings\n",
    "        \"\"\"\n",
    "\n",
    "        # First find all word IDs of all words in all sentences in the batch\n",
    "        # and the character IDs of all characters in all words in all sentences\n",
    "        word_ids = [self.word_to_id[word if word in self.word_to_id else self.unknown_word] for seq in x for word in seq]\n",
    "        char_ids = [[self.char_to_id[char] for char in word] for seq in x for word in seq]\n",
    "\n",
    "        # The 'to(self.device)' below is necessary for making sure that\n",
    "        # the model and the data are on the same device (CPU or CUDA).\n",
    "        word_tensor = torch.tensor(word_ids).to(self.device)\n",
    "        char_tensor = torch.tensor(char_ids).to(self.device)\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Get character-level embeddings and pass them through the character-level biRNN\n",
    "        char_embeds = self.char_emb(char_tensor)\n",
    "        char_embeds = torch.nn.functional.pad(char_embeds, (1, 0), \"constant\", 0)  # Pad on the left\n",
    "        packed_char_embeds = torch.nn.utils.rnn.pack_padded_sequence(char_embeds, [len(seq) for seq in char_ids], batch_first=True)\n",
    "        char_outputs, _ = self.char_birnn(packed_char_embeds)\n",
    "        char_outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(char_outputs, batch_first=True)\n",
    "        char_outputs = char_outputs[:, :, :self.char_hidden_size] + char_outputs[:, :, self.char_hidden_size:]  # Sum if bidirectional\n",
    "\n",
    "        # Concatenate character-level and word-level embeddings and pass them through the word-level biRNN\n",
    "        word_embeds = self.word_emb(word_tensor)\n",
    "        combined_embeds = torch.cat((word_embeds, char_outputs), dim=-1)\n",
    "        packed_combined_embeds = torch.nn.utils.rnn.pack_padded_sequence(combined_embeds, [len(seq) for seq in x], batch_first=True)\n",
    "        word_outputs, _ = self.word_birnn(packed_combined_embeds)\n",
    "        word_outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(word_outputs, batch_first=True)\n",
    "        word_outputs = word_outputs[:, :, :self.word_hidden_size] + word_outputs[:, :, self.word_hidden_size:]  # Sum if bidirectional\n",
    "\n",
    "        # Pass the outputs through the final linear layer to get the logits\n",
    "        logits = self.final_pred(word_outputs)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e3108f77-1bc5-4ac1-96fa-380d6c02a8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/340 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 13 at dim 1 (got 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m tqdm(training_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m     23\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 24\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(logits\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, logits\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]), torch\u001b[38;5;241m.\u001b[39mtensor(y)\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,))\n\u001b[1;32m     27\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 85\u001b[0m, in \u001b[0;36mNERClassifier.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;66;03m# The 'to(self.device)' below is necessary for making sure that\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# the model and the data are on the same device (CPU or CUDA).\u001b[39;00m\n\u001b[1;32m     84\u001b[0m word_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(word_ids)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 85\u001b[0m char_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchar_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;66;03m# YOUR CODE HERE\u001b[39;00m\n\u001b[1;32m     88\u001b[0m \n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# Get character-level embeddings and pass them through the character-level biRNN\u001b[39;00m\n\u001b[1;32m     90\u001b[0m char_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchar_emb(char_tensor)\n",
      "\u001b[0;31mValueError\u001b[0m: expected sequence of length 13 at dim 1 (got 8)"
     ]
    }
   ],
   "source": [
    "# ================== Hyper-parameters ==================== #\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "\n",
    "# ======================= Training ======================= #\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print( \"Running on\", device )\n",
    "\n",
    "dim, word_to_id, embeddings = load_glove_embeddings('/datasets/dd2417/glove.6B.50d.txt')\n",
    "training_set = NERDataset('/datasets/dd2417/ner_training.csv', word_to_id)\n",
    "training_loader = DataLoader(training_set, batch_size=128, collate_fn=pad_sequence)\n",
    "\n",
    "ner = NERClassifier(embeddings, char_to_id, word_to_id, device=device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(ner.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "ner.train()\n",
    "for epoch in range(epochs):   \n",
    "    for x, y in tqdm(training_loader, desc=\"Epoch {}\".format(epoch + 1)):\n",
    "        optimizer.zero_grad()\n",
    "        logits = ner(x)\n",
    "            \n",
    "        loss = criterion(logits.reshape(-1, logits.shape[2]), torch.tensor(y).to(device).reshape(-1,))\n",
    "        loss.backward()\n",
    "        \n",
    "        clip_grad_norm_(ner.parameters(), 5)\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a1b98aa-695d-4f95-96b9-a4a0d5046cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting terminaltables\n",
      "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
      "Installing collected packages: terminaltables\n",
      "Successfully installed terminaltables-3.1.10\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'ner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minstall terminaltables\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mterminaltables\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AsciiTable\n\u001b[0;32m----> 6\u001b[0m \u001b[43mner\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m confusion_matrix \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m      8\u001b[0m                     [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m      9\u001b[0m test_set \u001b[38;5;241m=\u001b[39m NERDataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/datasets/dd2417/ner_test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, word_to_id)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ner' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "import numpy as np\n",
    "%pip install terminaltables\n",
    "from terminaltables import AsciiTable\n",
    "\n",
    "ner.eval()\n",
    "confusion_matrix = [[0, 0],\n",
    "                    [0, 0]]\n",
    "test_set = NERDataset('/datasets/dd2417/ner_test.csv', word_to_id)\n",
    "for x, y in test_set:\n",
    "    pred = torch.argmax(ner([x]), dim=-1).detach().cpu().numpy().reshape(-1,)\n",
    "    y = np.array(y)\n",
    "    tp = np.sum(pred[y == 1])\n",
    "    tn = np.sum(1 - pred[y == 0])\n",
    "    fp = np.sum(1 - y[pred == 1])\n",
    "    fn = np.sum(y[pred == 0])\n",
    "\n",
    "    confusion_matrix[0][0] += tn\n",
    "    confusion_matrix[1][1] += tp\n",
    "        \n",
    "    confusion_matrix[0][1] += fp\n",
    "    confusion_matrix[1][0] += fn\n",
    "\n",
    "    \n",
    "table = [['', 'Predicted no name', 'Predicted name'],\n",
    "             ['Real no name', confusion_matrix[0][0], confusion_matrix[0][1]],\n",
    "             ['Real name', confusion_matrix[1][0], confusion_matrix[1][1]]]\n",
    "\n",
    "t = AsciiTable(table)\n",
    "print(t.table)\n",
    "print(\"Accuracy: {}\".format(\n",
    "    round((confusion_matrix[0][0] + confusion_matrix[1][1]) / np.sum(confusion_matrix), 4))\n",
    ")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
